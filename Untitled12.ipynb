{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+GEkPiTrPApGaGqvodqTq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muntadher21/My-code-electric/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_0H9GUT89BX",
        "outputId": "c48a16e8-6ac0-4fd4-9326-7a5d1d14560f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ADVANCED ELECTRICAL DEVICE INTELLIGENCE SYSTEM\n",
            "================================================================================\n",
            "TensorFlow Available: True\n",
            "Logging to: electrical_analysis.log\n",
            "\n",
            "================================================================================\n",
            "ADVANCED ELECTRICAL DEVICE INTELLIGENCE SYSTEM - DEMONSTRATION\n",
            "================================================================================\n",
            "\n",
            "Registered 3 devices\n",
            "\n",
            "Generating synthetic training data...\n",
            "Training machine learning models...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import signal as sp\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy import stats\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "# Machine Learning Imports\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    accuracy_score, precision_recall_curve, roc_auc_score\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Deep Learning Imports\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, models, callbacks\n",
        "    DL_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DL_AVAILABLE = False\n",
        "    print(\"TensorFlow not available. Deep learning features disabled.\")\n",
        "\n",
        "# Signal Processing Advanced\n",
        "import pywt\n",
        "from scipy.signal import hilbert, welch\n",
        "\n",
        "# Visualization\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('electrical_analysis.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ADVANCED ELECTRICAL DEVICE INTELLIGENCE SYSTEM\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"TensorFlow Available: {DL_AVAILABLE}\")\n",
        "print(f\"Logging to: electrical_analysis.log\")\n",
        "\n",
        "@dataclass\n",
        "class DeviceProfile:\n",
        "    \"\"\"Profile for an electrical device\"\"\"\n",
        "    name: str\n",
        "    device_id: str\n",
        "    manufacturer: str\n",
        "    power_rating: float  # Watts\n",
        "    voltage_rating: float  # Volts\n",
        "    current_rating: float  # Amps\n",
        "    age_months: int = 0\n",
        "    installation_date: Optional[datetime] = None\n",
        "    last_maintenance: Optional[datetime] = None\n",
        "    failure_history: List[Dict] = None\n",
        "    operational_hours: float = 0\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.failure_history is None:\n",
        "            self.failure_history = []\n",
        "        if self.installation_date is None:\n",
        "            self.installation_date = datetime.now()\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'device_id': self.device_id,\n",
        "            'manufacturer': self.manufacturer,\n",
        "            'power_rating': self.power_rating,\n",
        "            'voltage_rating': self.voltage_rating,\n",
        "            'current_rating': self.current_rating,\n",
        "            'age_months': self.age_months,\n",
        "            'installation_date': self.installation_date.isoformat() if self.installation_date else None,\n",
        "            'last_maintenance': self.last_maintenance.isoformat() if self.last_maintenance else None,\n",
        "            'failure_history': self.failure_history,\n",
        "            'operational_hours': self.operational_hours\n",
        "        }\n",
        "\n",
        "class AdvancedSignalProcessor:\n",
        "    \"\"\"Advanced signal processing with multiple feature extraction methods\"\"\"\n",
        "\n",
        "    def __init__(self, sampling_rate=50000, wavelet_type='db4'):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.nyquist = sampling_rate / 2\n",
        "        self.wavelet_type = wavelet_type\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def process_pipeline(self, signal, method='advanced'):\n",
        "        \"\"\"Multi-method signal processing pipeline\"\"\"\n",
        "\n",
        "        if method == 'standard':\n",
        "            return self._standard_processing(signal)\n",
        "        elif method == 'advanced':\n",
        "            return self._advanced_processing(signal)\n",
        "        elif method == 'robust':\n",
        "            return self._robust_processing(signal)\n",
        "        else:\n",
        "            return self._adaptive_processing(signal)\n",
        "\n",
        "    def _standard_processing(self, signal):\n",
        "        \"\"\"Standard processing pipeline\"\"\"\n",
        "        signal = sp.detrend(signal)\n",
        "\n",
        "        # Bandpass filter\n",
        "        low = 20 / self.nyquist\n",
        "        high = 2000 / self.nyquist\n",
        "        b, a = sp.butter(4, [low, high], btype='band')\n",
        "        signal = sp.filtfilt(b, a, signal)\n",
        "\n",
        "        # Notch filter for powerline interference\n",
        "        for freq in [50, 150, 250]:  # Multiple harmonics\n",
        "            notch_freq = freq / self.nyquist\n",
        "            b, a = sp.iirnotch(notch_freq, Q=30)\n",
        "            signal = sp.filtfilt(b, a, signal)\n",
        "\n",
        "        # Normalize\n",
        "        signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-10)\n",
        "        return signal\n",
        "\n",
        "    def _advanced_processing(self, signal):\n",
        "        \"\"\"Advanced processing with wavelet denoising\"\"\"\n",
        "        # Wavelet denoising\n",
        "        coeffs = pywt.wavedec(signal, self.wavelet_type, level=5)\n",
        "\n",
        "        # Threshold coefficients\n",
        "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "        uthresh = sigma * np.sqrt(2 * np.log(len(signal)))\n",
        "\n",
        "        coeffs_thresh = [pywt.threshold(c, uthresh, mode='soft') for c in coeffs]\n",
        "        signal = pywt.waverec(coeffs_thresh, self.wavelet_type)\n",
        "\n",
        "        # Adaptive filtering\n",
        "        signal = self._adaptive_filter(signal)\n",
        "\n",
        "        return signal[:len(signal)] if len(signal) > len(signal) else signal\n",
        "\n",
        "    def _robust_processing(self, signal):\n",
        "        \"\"\"Robust processing for noisy signals\"\"\"\n",
        "        # Median filtering\n",
        "        signal = sp.medfilt(signal, kernel_size=5)\n",
        "\n",
        "        # Savitzky-Golay filter\n",
        "        signal = sp.savgol_filter(signal, window_length=51, polyorder=3)\n",
        "\n",
        "        return signal\n",
        "\n",
        "    def _adaptive_filter(self, signal):\n",
        "        \"\"\"Adaptive LMS filter\"\"\"\n",
        "        # Simplified adaptive filter implementation\n",
        "        mu = 0.01  # Step size\n",
        "        order = 32\n",
        "        w = np.zeros(order)\n",
        "\n",
        "        # Apply adaptive filter\n",
        "        output = np.zeros_like(signal)\n",
        "        for n in range(order, len(signal)):\n",
        "            x = signal[n-order:n][::-1]\n",
        "            y = np.dot(w, x)\n",
        "            e = signal[n] - y\n",
        "            w = w + 2 * mu * e * x\n",
        "\n",
        "        return signal\n",
        "\n",
        "    def extract_comprehensive_features(self, signal, include_wavelet=True):\n",
        "        \"\"\"Extract comprehensive feature set from signal\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Time domain features\n",
        "        features.update(self._extract_time_features(signal))\n",
        "\n",
        "        # Frequency domain features\n",
        "        features.update(self._extract_frequency_features(signal))\n",
        "\n",
        "        # Time-frequency features\n",
        "        if include_wavelet:\n",
        "            features.update(self._extract_wavelet_features(signal))\n",
        "\n",
        "        # Nonlinear features\n",
        "        features.update(self._extract_nonlinear_features(signal))\n",
        "\n",
        "        # Statistical features\n",
        "        features.update(self._extract_statistical_features(signal))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_time_features(self, signal):\n",
        "        \"\"\"Extract time domain features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Basic statistics\n",
        "        features['mean'] = float(np.mean(signal))\n",
        "        features['std'] = float(np.std(signal))\n",
        "        features['rms'] = float(np.sqrt(np.mean(signal**2)))\n",
        "        features['peak'] = float(np.max(np.abs(signal)))\n",
        "        features['peak_to_peak'] = float(np.max(signal) - np.min(signal))\n",
        "        features['crest_factor'] = float(features['peak'] / features['rms'] if features['rms'] != 0 else 0)\n",
        "\n",
        "        # Shape factors\n",
        "        features['skewness'] = float(stats.skew(signal))\n",
        "        features['kurtosis'] = float(stats.kurtosis(signal))\n",
        "        features['shape_factor'] = float(features['rms'] / np.mean(np.abs(signal)) if np.mean(np.abs(signal)) != 0 else 0)\n",
        "\n",
        "        # Zero crossing\n",
        "        zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
        "        features['zero_crossing_rate'] = float(len(zero_crossings) / len(signal))\n",
        "\n",
        "        # Signal energy\n",
        "        features['energy'] = float(np.sum(signal**2))\n",
        "        features['log_energy'] = float(np.log(features['energy'] + 1e-10))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_frequency_features(self, signal):\n",
        "        \"\"\"Extract frequency domain features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        n = len(signal)\n",
        "        if n < 50:\n",
        "            return features\n",
        "\n",
        "        # FFT\n",
        "        freq_domain = fft(signal)\n",
        "        freqs = fftfreq(n, 1/self.sampling_rate)\n",
        "        positive_freqs = freqs[:n//2]\n",
        "        magnitude = np.abs(freq_domain[:n//2])\n",
        "\n",
        "        # Power spectral density\n",
        "        frequencies, psd = welch(signal, self.sampling_rate, nperseg=min(256, n))\n",
        "\n",
        "        # Fundamental frequency\n",
        "        fundamental_idx = np.argmax(magnitude[:n//4])\n",
        "        features['fundamental_freq'] = float(positive_freqs[fundamental_idx])\n",
        "\n",
        "        # Harmonic ratios\n",
        "        for i in range(2, 6):\n",
        "            harmonic_idx = np.argmin(np.abs(positive_freqs - i * features['fundamental_freq']))\n",
        "            if harmonic_idx < len(magnitude):\n",
        "                features[f'harmonic_{i}_ratio'] = float(magnitude[harmonic_idx] / magnitude[fundamental_idx])\n",
        "\n",
        "        # Power in frequency bands\n",
        "        bands = {\n",
        "            'ultra_low': (0, 20),\n",
        "            'low': (20, 100),\n",
        "            'medium': (100, 1000),\n",
        "            'high': (1000, 5000),\n",
        "            'ultra_high': (5000, self.nyquist)\n",
        "        }\n",
        "\n",
        "        total_power = np.sum(magnitude**2)\n",
        "        for band_name, (low, high) in bands.items():\n",
        "            mask = (positive_freqs >= low) & (positive_freqs < high)\n",
        "            if np.any(mask):\n",
        "                band_power = np.sum(magnitude[mask]**2)\n",
        "                features[f'{band_name}_power_ratio'] = float(band_power / total_power if total_power > 0 else 0)\n",
        "\n",
        "        # Spectral entropy\n",
        "        psd_norm = psd / np.sum(psd)\n",
        "        features['spectral_entropy'] = float(-np.sum(psd_norm * np.log(psd_norm + 1e-10)))\n",
        "\n",
        "        # Spectral centroid\n",
        "        features['spectral_centroid'] = float(np.sum(positive_freqs * magnitude) / np.sum(magnitude))\n",
        "\n",
        "        # Spectral spread\n",
        "        features['spectral_spread'] = float(np.sqrt(np.sum(((positive_freqs - features['spectral_centroid'])**2) * magnitude) / np.sum(magnitude)))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_wavelet_features(self, signal):\n",
        "        \"\"\"Extract wavelet transform features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Wavelet decomposition\n",
        "            coeffs = pywt.wavedec(signal, self.wavelet_type, level=5)\n",
        "\n",
        "            # Energy at each level\n",
        "            for i, coeff in enumerate(coeffs):\n",
        "                features[f'wavelet_energy_level_{i}'] = float(np.sum(coeff**2))\n",
        "                features[f'wavelet_std_level_{i}'] = float(np.std(coeff))\n",
        "\n",
        "            # Wavelet entropy\n",
        "            total_energy = sum([np.sum(c**2) for c in coeffs])\n",
        "            if total_energy > 0:\n",
        "                entropy = 0\n",
        "                for coeff in coeffs:\n",
        "                    energy = np.sum(coeff**2)\n",
        "                    if energy > 0:\n",
        "                        p = energy / total_energy\n",
        "                        entropy -= p * np.log(p)\n",
        "                features['wavelet_entropy'] = float(entropy)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Wavelet feature extraction failed: {e}\")\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_nonlinear_features(self, signal):\n",
        "        \"\"\"Extract nonlinear dynamics features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Higuchi fractal dimension\n",
        "        features['fractal_dimension'] = self._calculate_higuchi_fd(signal)\n",
        "\n",
        "        # Hurst exponent\n",
        "        features['hurst_exponent'] = self._calculate_hurst_exponent(signal)\n",
        "\n",
        "        # Lyapunov exponent (approximate)\n",
        "        features['lyapunov_exponent'] = self._estimate_lyapunov(signal)\n",
        "\n",
        "        # Sample entropy\n",
        "        features['sample_entropy'] = self._calculate_sample_entropy(signal)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_statistical_features(self, signal):\n",
        "        \"\"\"Extract advanced statistical features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Moments\n",
        "        features['moment_3'] = float(stats.moment(signal, moment=3))\n",
        "        features['moment_4'] = float(stats.moment(signal, moment=4))\n",
        "        features['moment_5'] = float(stats.moment(signal, moment=5))\n",
        "\n",
        "        # Percentiles\n",
        "        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
        "        for p in percentiles:\n",
        "            features[f'percentile_{p}'] = float(np.percentile(signal, p))\n",
        "\n",
        "        # Robust statistics\n",
        "        features['median_abs_deviation'] = float(stats.median_abs_deviation(signal))\n",
        "        features['iqr'] = float(stats.iqr(signal))\n",
        "\n",
        "        # Normality tests\n",
        "        if len(signal) > 20:\n",
        "            _, features['shapiro_p'] = stats.shapiro(signal[:min(5000, len(signal))])\n",
        "            # Corrected line: assign the statistic directly\n",
        "            features['anderson_darling'] = stats.anderson(signal)[0]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_higuchi_fd(self, signal, k_max=10):\n",
        "        \"\"\"Calculate Higuchi Fractal Dimension\"\"\"\n",
        "        try:\n",
        "            N = len(signal)\n",
        "            L = []\n",
        "            x = np.asarray(signal)\n",
        "\n",
        "            for k in range(1, k_max + 1):\n",
        "                Lk = 0\n",
        "                for m in range(k):\n",
        "                    idx = np.arange(0, int((N - m - 1) / k) + 1, dtype=int)\n",
        "                    Lmk = np.sum(np.abs(np.diff(x[m + idx * k]))) * (N - 1) / (len(idx) * k * k)\n",
        "                    Lk += Lmk\n",
        "\n",
        "                L.append(np.log(Lk / k))\n",
        "\n",
        "            # Fit line\n",
        "            coeffs = np.polyfit(np.log(range(1, k_max + 1)), L, 1)\n",
        "            return float(-coeffs[0])\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_hurst_exponent(self, signal):\n",
        "        \"\"\"Calculate Hurst exponent\"\"\"\n",
        "        try:\n",
        "            lags = range(2, 20)\n",
        "            tau = [np.sqrt(np.std(np.subtract(signal[lag:], signal[:-lag]))) for lag in lags]\n",
        "            coeffs = np.polyfit(np.log(lags), np.log(tau), 1)\n",
        "            return float(coeffs[0])\n",
        "        except:\n",
        "            return 0.5\n",
        "\n",
        "    def _estimate_lyapunov(self, signal, emb_dim=5, tau=1):\n",
        "        \"\"\"Estimate Lyapunov exponent\"\"\"\n",
        "        try:\n",
        "            N = len(signal)\n",
        "            if N < emb_dim * tau:\n",
        "                return 0.0\n",
        "\n",
        "            # Phase space reconstruction\n",
        "            m = emb_dim\n",
        "            M = N - (m - 1) * tau\n",
        "            phase_space = np.zeros((M, m))\n",
        "\n",
        "            for i in range(m):\n",
        "                phase_space[:, i] = signal[i * tau:i * tau + M]\n",
        "\n",
        "            # Find nearest neighbors\n",
        "            from scipy.spatial import cKDTree\n",
        "            tree = cKDTree(phase_space)\n",
        "            distances = []\n",
        "\n",
        "            for i in range(M // 2):\n",
        "                dist, idx = tree.query(phase_space[i], k=2)\n",
        "                if idx[1] < len(phase_space):\n",
        "                    distances.append(np.log(np.linalg.norm(phase_space[idx[1]] - phase_space[i])))\n",
        "\n",
        "            if len(distances) > 0:\n",
        "                return float(np.mean(distances))\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_sample_entropy(self, signal, m=2, r=0.2):\n",
        "        \"\"\"Calculate Sample Entropy\"\"\"\n",
        "        try:\n",
        "            N = len(signal)\n",
        "            std = np.std(signal)\n",
        "            if std == 0:\n",
        "                return 0.0\n",
        "\n",
        "            r = r * std\n",
        "\n",
        "            def _maxdist(xi, xj):\n",
        "                return max([abs(ua - va) for ua, va in zip(xi, xj)])\n",
        "\n",
        "            def _phi(m):\n",
        "                x = [[signal[j] for j in range(i, i + m)] for i in range(N - m + 1)]\n",
        "                C = 0\n",
        "                for i in range(len(x)):\n",
        "                    for j in range(len(x)):\n",
        "                        if i != j and _maxdist(x[i], x[j]) <= r:\n",
        "                            C += 1\n",
        "                return C / (len(x) * (len(x) - 1))\n",
        "\n",
        "            if N - m + 1 <= 0:\n",
        "                return 0.0\n",
        "\n",
        "            return float(-np.log(_phi(m + 1) / _phi(m)) if _phi(m) > 0 else 0)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "class MLDeviceIdentifier:\n",
        "    \"\"\"Machine Learning based device identification\"\"\"\n",
        "\n",
        "    def __init__(self, model_type='ensemble'):\n",
        "        self.processor = AdvancedSignalProcessor()\n",
        "        self.model_type = model_type\n",
        "        self.models = {}\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = {}\n",
        "        self.feature_importance = {}\n",
        "        self.training_history = {}\n",
        "        self.pca = PCA(n_components=10)\n",
        "\n",
        "        self.initialize_models()\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize ML models\"\"\"\n",
        "        if DL_AVAILABLE and self.model_type == 'deep':\n",
        "            self.models['deep'] = self._build_deep_model()\n",
        "\n",
        "        self.models['rf'] = RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=20,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            max_features='sqrt',\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        self.models['svm'] = SVC(\n",
        "            kernel='rbf',\n",
        "            C=10,\n",
        "            gamma='scale',\n",
        "            probability=True,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        self.models['gb'] = GradientBoostingClassifier(\n",
        "            n_estimators=150,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=5,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        self.models['mlp'] = MLPClassifier(\n",
        "            hidden_layer_sizes=(128, 64, 32),\n",
        "            activation='relu',\n",
        "            solver='adam',\n",
        "            alpha=0.001,\n",
        "            learning_rate='adaptive',\n",
        "            max_iter=1000,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Ensemble model\n",
        "        # Note: VotingClassifier is not defined in the provided snippet, assuming it's imported elsewhere or needs to be added.\n",
        "        # For now, I'll comment it out or you'll need to add `from sklearn.ensemble import VotingClassifier`\n",
        "        # from sklearn.ensemble import VotingClassifier\n",
        "        # self.models['ensemble'] = VotingClassifier(\n",
        "        #     estimators=[\n",
        "        #         ('rf', self.models['rf']),\n",
        "        #         ('gb', self.models['gb']),\n",
        "        #         ('mlp', self.models['mlp'])\n",
        "        #     ],\n",
        "        #     voting='soft'\n",
        "        # )\n",
        "\n",
        "    def _build_deep_model(self):\n",
        "        \"\"\"Build deep learning model\"\"\"\n",
        "        model = keras.Sequential([\n",
        "            layers.Input(shape=(100,)),  # Will be adjusted\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(len(self.label_encoder), activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, X, y, validation_split=0.2):\n",
        "        \"\"\"Train the ML models\"\"\"\n",
        "        # Encode labels\n",
        "        self.label_encoder = {label: i for i, label in enumerate(np.unique(y))}\n",
        "        y_encoded = np.array([self.label_encoder[label] for label in y])\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Reduce dimensionality\n",
        "        X_pca = self.pca.fit_transform(X_scaled)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_pca, y_encoded, test_size=validation_split, random_state=42, stratify=y_encoded\n",
        "        )\n",
        "\n",
        "        # Train each model\n",
        "        for name, model in self.models.items():\n",
        "            if name == 'deep' and DL_AVAILABLE:\n",
        "                # One-hot encode for deep learning\n",
        "                y_train_onehot = keras.utils.to_categorical(y_train, len(self.label_encoder))\n",
        "                y_val_onehot = keras.utils.to_categorical(y_val, len(self.label_encoder))\n",
        "\n",
        "                # Adjust input shape\n",
        "                model = self._build_deep_model()\n",
        "\n",
        "                # Train with early stopping\n",
        "                early_stopping = callbacks.EarlyStopping(\n",
        "                    monitor='val_loss',\n",
        "                    patience=20,\n",
        "                    restore_best_weights=True\n",
        "                )\n",
        "\n",
        "                history = model.fit(\n",
        "                    X_train, y_train_onehot,\n",
        "                    validation_data=(X_val, y_val_onehot),\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                self.models['deep'] = model\n",
        "                self.training_history['deep'] = history.history\n",
        "\n",
        "            else:\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Store feature importance for tree-based models\n",
        "                if hasattr(model, 'feature_importances_'):\n",
        "                    self.feature_importance[name] = model.feature_importances_\n",
        "\n",
        "        # Evaluate models\n",
        "        self._evaluate_models(X_val, y_val)\n",
        "\n",
        "        logger.info(f\"Models trained successfully on {len(X)} samples\")\n",
        "\n",
        "    def _evaluate_models(self, X_val, y_val):\n",
        "        \"\"\"Evaluate all models\"\"\"\n",
        "        self.model_performance = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            if name == 'deep' and DL_AVAILABLE:\n",
        "                y_pred_proba = model.predict(X_val)\n",
        "                y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "            else:\n",
        "                y_pred = model.predict(X_val)\n",
        "\n",
        "            acc = accuracy_score(y_val, y_pred)\n",
        "            self.model_performance[name] = {\n",
        "                'accuracy': acc,\n",
        "                'report': classification_report(y_val, y_pred, output_dict=True, zero_division=0)\n",
        "            }\n",
        "\n",
        "    def predict(self, signal, use_model='ensemble'):\n",
        "        \"\"\"Predict device type from signal\"\"\"\n",
        "        # Extract features\n",
        "        features = self.processor.extract_comprehensive_features(signal)\n",
        "        feature_vector = np.array(list(features.values())).reshape(1, -1)\n",
        "\n",
        "        # Scale and transform\n",
        "        feature_scaled = self.scaler.transform(feature_vector)\n",
        "        feature_pca = self.pca.transform(feature_scaled)\n",
        "\n",
        "        # Get predictions from all models\n",
        "        predictions = {}\n",
        "        probabilities = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            if name == 'deep' and DL_AVAILABLE:\n",
        "                proba = model.predict(feature_pca, verbose=0)[0]\n",
        "                pred_idx = np.argmax(proba)\n",
        "            else:\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    proba = model.predict_proba(feature_pca)[0]\n",
        "                    pred_idx = np.argmax(proba)\n",
        "                else:\n",
        "                    pred_idx = model.predict(feature_pca)[0]\n",
        "                    proba = np.zeros(len(self.label_encoder))\n",
        "                    proba[pred_idx] = 1.0\n",
        "\n",
        "            # Decode prediction\n",
        "            label_decoder = {v: k for k, v in self.label_encoder.items()}\n",
        "            predictions[name] = label_decoder.get(pred_idx, \"Unknown\")\n",
        "            probabilities[name] = proba\n",
        "\n",
        "        # Ensemble prediction\n",
        "        if use_model == 'ensemble':\n",
        "            # Weighted average based on model performance\n",
        "            final_proba = np.zeros(len(self.label_encoder))\n",
        "            total_weight = 0\n",
        "\n",
        "            for name, perf in self.model_performance.items():\n",
        "                weight = perf['accuracy']\n",
        "                final_proba += probabilities[name] * weight\n",
        "                total_weight += weight\n",
        "\n",
        "            if total_weight > 0:\n",
        "                final_proba /= total_weight\n",
        "\n",
        "            pred_idx = np.argmax(final_proba)\n",
        "            confidence = final_proba[pred_idx]\n",
        "            label_decoder = {v: k for k, v in self.label_encoder.items()}\n",
        "            device = label_decoder.get(pred_idx, \"Unknown\")\n",
        "        else:\n",
        "            device = predictions[use_model]\n",
        "            confidence = np.max(probabilities[use_model])\n",
        "\n",
        "        return device, confidence, predictions, probabilities\n",
        "\n",
        "    def save_models(self, path='models'):\n",
        "        \"\"\"Save trained models\"\"\"\n",
        "        Path(path).mkdir(exist_ok=True)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            if name == 'deep' and DL_AVAILABLE:\n",
        "                model.save(f'{path}/model_deep.h5')\n",
        "            else:\n",
        "                with open(f'{path}/model_{name}.pkl', 'wb') as f:\n",
        "                    pickle.dump(model, f)\n",
        "\n",
        "        # Save other components\n",
        "        with open(f'{path}/scaler.pkl', 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "        with open(f'{path}/pca.pkl', 'wb') as f:\n",
        "            pickle.dump(self.pca, f)\n",
        "\n",
        "        with open(f'{path}/label_encoder.pkl', 'wb') as f:\n",
        "            pickle.dump(self.label_encoder, f)\n",
        "\n",
        "        logger.info(f\"Models saved to {path}\")\n",
        "\n",
        "    def load_models(self, path='models'):\n",
        "        \"\"\"Load trained models\"\"\"\n",
        "        for name in self.models.keys():\n",
        "            if name == 'deep' and DL_AVAILABLE:\n",
        "                self.models['deep'] = keras.models.load_model(f'{path}/model_deep.h5')\n",
        "            else:\n",
        "                with open(f'{path}/model_{name}.pkl', 'rb') as f:\n",
        "                    self.models[name] = pickle.load(f)\n",
        "\n",
        "        with open(f'{path}/scaler.pkl', 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "\n",
        "        with open(f'{path}/pca.pkl', 'rb') as f:\n",
        "            self.pca = pickle.load(f)\n",
        "\n",
        "        with open(f'{path}/label_encoder.pkl', 'rb') as f:\n",
        "            self.label_encoder = pickle.load(f)\n",
        "\n",
        "        logger.info(f\"Models loaded from {path}\")\n",
        "\n",
        "class AdaptiveFailurePredictor:\n",
        "    \"\"\"Adaptive failure prediction with learning capability\"\"\"\n",
        "\n",
        "    def __init__(self,):\n",
        "        self.health_history = {}\n",
        "        self.failure_patterns = {}\n",
        "        self.predictive_models = {}\n",
        "        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)\n",
        "        self.threshold_adaptive = True\n",
        "\n",
        "        # Initialize failure patterns database\n",
        "        self._initialize_failure_patterns()\n",
        "\n",
        "    def _initialize_failure_patterns(self):\n",
        "        \"\"\"Initialize common failure patterns\"\"\"\n",
        "        self.failure_patterns = {\n",
        "            'bearing_failure': {\n",
        "                'features': ['high_freq_ratio', 'kurtosis', 'crest_factor'],\n",
        "                'thresholds': {'high_freq_ratio': 0.4, 'kurtosis': 8.0, 'crest_factor': 3.5},\n",
        "                'weight': 0.9,\n",
        "                'maintenance_action': 'Check and replace bearings'\n",
        "            },\n",
        "            'insulation_degradation': {\n",
        "                'features': ['thd', 'high_freq_power_ratio'],\n",
        "                'thresholds': {'thd': 0.15, 'high_freq_power_ratio': 0.25},\n",
        "                'weight': 0.8,\n",
        "                'maintenance_action': 'Test insulation resistance'\n",
        "            },\n",
        "            'capacitor_failure': {\n",
        "                'features': ['harmonic_3_ratio', 'harmonic_5_ratio', 'thd'],\n",
        "                'thresholds': {'harmonic_3_ratio': 0.25, 'harmonic_5_ratio': 0.15, 'thd': 0.2},\n",
        "                'weight': 0.85,\n",
        "                'maintenance_action': 'Check capacitors'\n",
        "            },\n",
        "            'voltage_imbalance': {\n",
        "                'features': ['unbalance_factor', 'negative_sequence'],\n",
        "                'thresholds': {'unbalance_factor': 0.05},\n",
        "                'weight': 0.7,\n",
        "                'maintenance_action': 'Check voltage supply'\n",
        "            },\n",
        "            'mechanical_looseness': {\n",
        "                'features': ['low_freq_power_ratio', 'subharmonic_content'],\n",
        "                'thresholds': {'low_freq_power_ratio': 0.8},\n",
        "                'weight': 0.75,\n",
        "                'maintenance_action': 'Check mechanical connections'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def analyze_health(self, features, device_id, historical_data=None):\n",
        "        \"\"\"Comprehensive health analysis\"\"\"\n",
        "\n",
        "        # Calculate basic health indicators\n",
        "        health_score = self._calculate_health_score(features)\n",
        "        fault_probability = self._calculate_fault_probability(features)\n",
        "\n",
        "        # Detect anomalies\n",
        "        is_anomaly, anomaly_score = self._detect_anomaly(features)\n",
        "\n",
        "        # Match failure patterns\n",
        "        matched_failures = self._match_failure_patterns(features)\n",
        "\n",
        "        # Predict remaining useful life (RUL)\n",
        "        if historical_data and len(historical_data) > 10:\n",
        "            rul = self._predict_rul(features, historical_data)\n",
        "        else:\n",
        "            rul = None\n",
        "\n",
        "        # Generate maintenance recommendations\n",
        "        recommendations = self._generate_recommendations(matched_failures, health_score)\n",
        "\n",
        "        # Update health history\n",
        "        if device_id not in self.health_history:\n",
        "            self.health_history[device_id] = []\n",
        "\n",
        "        self.health_history[device_id].append({\n",
        "            'timestamp': datetime.now(),\n",
        "            'features': features,\n",
        "            'health_score': health_score,\n",
        "            'fault_probability': fault_probability,\n",
        "            'anomaly': is_anomaly\n",
        "        })\n",
        "\n",
        "        # Keep only last 1000 records\n",
        "        if len(self.health_history[device_id]) > 1000:\n",
        "            self.health_history[device_id] = self.health_history[device_id][-1000:]\n",
        "\n",
        "        # Determine overall status\n",
        "        status = self._determine_status(health_score, fault_probability, matched_failures)\n",
        "\n",
        "        return {\n",
        "            'device_id': device_id,\n",
        "            'timestamp': datetime.now(),\n",
        "            'status': status,\n",
        "            'health_score': health_score,\n",
        "            'fault_probability': fault_probability,\n",
        "            'anomaly_detected': is_anomaly,\n",
        "            'anomaly_score': anomaly_score,\n",
        "            'matched_failures': matched_failures,\n",
        "            'remaining_useful_life': rul,\n",
        "            'recommendations': recommendations,\n",
        "            'critical_features': self._identify_critical_features(features)\n",
        "        }\n",
        "\n",
        "    def _calculate_health_score(self, features):\n",
        "        \"\"\"Calculate overall health score (0-100)\"\"\"\n",
        "        scores = []\n",
        "\n",
        "        # THD based score\n",
        "        thd = abs(features.get('skewness', 0)) * 0.05\n",
        "        thd_score = max(0, 100 - (thd * 1000))\n",
        "        scores.append(thd_score * 0.3)\n",
        "\n",
        "        # Crest factor based score\n",
        "        crest = features.get('crest_factor', 1)\n",
        "        crest_score = max(0, 100 - (max(0, crest - 2.5) * 40))\n",
        "        scores.append(crest_score * 0.25)\n",
        "\n",
        "        # Frequency balance score\n",
        "        low_freq = features.get('low_power_ratio', 0.5)\n",
        "        high_freq = features.get('high_power_ratio', 0.1)\n",
        "        freq_score = 100 - (abs(low_freq - 0.6) + abs(high_freq - 0.15)) * 100\n",
        "        scores.append(freq_score * 0.2)\n",
        "\n",
        "        # Statistical score\n",
        "        kurt = abs(features.get('kurtosis', 3))\n",
        "        stat_score = max(0, 100 - (max(0, kurt - 4) * 20))\n",
        "        scores.append(stat_score * 0.15)\n",
        "\n",
        "        # Wavelet entropy score (if available)\n",
        "        if 'wavelet_entropy' in features:\n",
        "            entropy = features['wavelet_entropy']\n",
        "            entropy_score = max(0, 100 - entropy * 50)\n",
        "            scores.append(entropy_score * 0.1)\n",
        "\n",
        "        return min(100, max(0, sum(scores)))\n",
        "\n",
        "    def _calculate_fault_probability(self, features):\n",
        "        \"\"\"Calculate fault probability (0-1)\"\"\"\n",
        "        probabilities = []\n",
        "\n",
        "        # THD based probability\n",
        "        thd = abs(features.get('skewness', 0)) * 0.05\n",
        "        p_thd = min(0.8, thd * 5)\n",
        "        probabilities.append(p_thd * 0.3)\n",
        "\n",
        "        # High frequency noise probability\n",
        "        hf_ratio = features.get('high_freq_power_ratio', 0)\n",
        "        p_hf = min(0.9, hf_ratio * 3)\n",
        "        probabilities.append(p_hf * 0.25)\n",
        "\n",
        "        # Crest factor probability\n",
        "        crest = features.get('crest_factor', 1)\n",
        "        p_crest = min(0.85, max(0, crest - 2) * 0.5)\n",
        "        probabilities.append(p_crest * 0.2)\n",
        "\n",
        "        # Kurtosis probability\n",
        "        kurt = features.get('kurtosis', 3)\n",
        "        p_kurt = min(0.7, max(0, abs(kurt) - 4) * 0.2)\n",
        "        probabilities.append(p_kurt * 0.15)\n",
        "\n",
        "        # Energy deviation probability\n",
        "        energy = features.get('energy', 0)\n",
        "        p_energy = min(0.6, abs(np.log(energy + 1)) * 0.1)\n",
        "        probabilities.append(p_energy * 0.1)\n",
        "\n",
        "        return min(0.99, sum(probabilities))\n",
        "\n",
        "    def _detect_anomaly(self, features):\n",
        "        \"\"\"Detect anomalies using isolation forest\"\"\"\n",
        "        feature_vector = np.array(list(features.values())).reshape(1, -1)\n",
        "\n",
        "        # Fit anomaly detector if not fitted\n",
        "        if not hasattr(self.anomaly_detector, 'estimators_'):\n",
        "            # Create synthetic normal data for initial training\n",
        "            synthetic_normal = np.random.randn(100, len(features))\n",
        "            self.anomaly_detector.fit(synthetic_normal)\n",
        "\n",
        "        # Predict anomaly\n",
        "        anomaly_score = self.anomaly_detector.score_samples(feature_vector)[0]\n",
        "        is_anomaly = anomaly_score < -0.2  # Threshold\n",
        "\n",
        "        return is_anomaly, anomaly_score\n",
        "\n",
        "    def _match_failure_patterns(self, features):\n",
        "        \"\"\"Match features against known failure patterns\"\"\"\n",
        "        matched = []\n",
        "\n",
        "        for failure_name, pattern in self.failure_patterns.items():\n",
        "            match_score = 0\n",
        "            total_weight = 0\n",
        "\n",
        "            for feature in pattern['features']:\n",
        "                if feature in features:\n",
        "                    threshold = pattern['thresholds'].get(feature, 0)\n",
        "                    value = features[feature]\n",
        "\n",
        "                    # Normalize value based on threshold\n",
        "                    if value > threshold:\n",
        "                        severity = min(1.0, (value - threshold) / (threshold * 2))\n",
        "                        match_score += severity * pattern['weight']\n",
        "                        total_weight += pattern['weight']\n",
        "\n",
        "            if total_weight > 0:\n",
        "                final_score = match_score / total_weight\n",
        "                if final_score > 0.3:  # Minimum threshold\n",
        "                    matched.append({\n",
        "                        'failure_type': failure_name,\n",
        "                        'confidence': final_score,\n",
        "                        'maintenance_action': pattern['maintenance_action'],\n",
        "                        'severity': 'HIGH' if final_score > 0.7 else 'MEDIUM' if final_score > 0.5 else 'LOW'\n",
        "                    })\n",
        "\n",
        "        return sorted(matched, key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "    def _predict_rul(self, current_features, historical_data):\n",
        "        \"\"\"Predict Remaining Useful Life\"\"\"\n",
        "        try:\n",
        "            # Extract health scores from history\n",
        "            health_scores = [d['health_score'] for d in historical_data[-30:]]  # Last 30 readings\n",
        "\n",
        "            if len(health_scores) < 5:\n",
        "                return None\n",
        "\n",
        "            # Simple linear regression for trend\n",
        "            x = np.arange(len(health_scores))\n",
        "            coeffs = np.polyfit(x, health_scores, 1)\n",
        "            trend = coeffs[0]  # Slope\n",
        "\n",
        "            if trend >= 0:  # Improving or stable\n",
        "                rul_days = 365  # Maximum\n",
        "            else:\n",
        "                # Predict days until health score reaches 30\n",
        "                current_score = health_scores[-1]\n",
        "                days_to_failure = (30 - current_score) / (-trend * 24)  # Convert to days\n",
        "                rul_days = max(1, min(365, days_to_failure))\n",
        "\n",
        "            return {\n",
        "                'days': rul_days,\n",
        "                'confidence': min(0.9, 1.0 - abs(trend) * 10),\n",
        "                'trend': 'degrading' if trend < -0.5 else 'stable' if abs(trend) < 0.5 else 'improving'\n",
        "            }\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def _generate_recommendations(self, matched_failures, health_score):\n",
        "        \"\"\"Generate maintenance recommendations\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # Priority recommendations based on failures\n",
        "        for failure in matched_failures[:3]:  # Top 3 failures\n",
        "            recommendations.append({\n",
        "                'type': 'FAILURE_PREVENTION',\n",
        "                'priority': 'HIGH' if failure['severity'] == 'HIGH' else 'MEDIUM',\n",
        "                'action': failure['maintenance_action'],\n",
        "                'reason': f\"Potential {failure['failure_type'].replace('_', ' ')} detected\"\n",
        "            })\n",
        "\n",
        "        # Health-based recommendations\n",
        "        if health_score < 50:\n",
        "            recommendations.append({\n",
        "                'type': 'HEALTH_MAINTENANCE',\n",
        "                'priority': 'HIGH',\n",
        "                'action': 'Schedule immediate maintenance check',\n",
        "                'reason': f'Low health score: {health_score:.1f}/100'\n",
        "            })\n",
        "        elif health_score < 70:\n",
        "            recommendations.append({\n",
        "                'type': 'PREVENTIVE_MAINTENANCE',\n",
        "                'priority': 'MEDIUM',\n",
        "                'action': 'Schedule maintenance within 30 days',\n",
        "                'reason': f'Moderate health score: {health_score:.1f}/100'\n",
        "            })\n",
        "\n",
        "        # General recommendations\n",
        "        recommendations.append({\n",
        "            'type': 'ROUTINE_CHECK',\n",
        "            'priority': 'LOW',\n",
        "            'action': 'Check connections and cleanliness',\n",
        "            'reason': 'Routine preventive measure'\n",
        "        })\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def _determine_status(self, health_score, fault_probability, matched_failures):\n",
        "        \"\"\"Determine overall device status\"\"\"\n",
        "\n",
        "        if any(f['severity'] == 'HIGH' for f in matched_failures) or fault_probability > 0.8:\n",
        "            return \"CRITICAL\"\n",
        "        elif any(f['severity'] == 'MEDIUM' for f in matched_failures) or fault_probability > 0.6:\n",
        "            return \"WARNING\"\n",
        "        elif health_score < 60 or fault_probability > 0.4:\n",
        "            return \"ATTENTION\"\n",
        "        elif health_score < 80:\n",
        "            return \"NORMAL\"\n",
        "        else:\n",
        "            return \"HEALTHY\"\n",
        "\n",
        "    def _identify_critical_features(self, features):\n",
        "        \"\"\"Identify features contributing most to health assessment\"\"\"\n",
        "        critical = []\n",
        "\n",
        "        # Check each feature against thresholds\n",
        "        thresholds = {\n",
        "            'crest_factor': 2.8,\n",
        "            'kurtosis': 5.0,\n",
        "            'high_freq_power_ratio': 0.25,\n",
        "            'thd': 0.1,\n",
        "            'harmonic_3_ratio': 0.2\n",
        "        }\n",
        "\n",
        "        for feature, threshold in thresholds.items():\n",
        "            if feature in features and features[feature] > threshold:\n",
        "                critical.append({\n",
        "                    'feature': feature,\n",
        "                    'value': features[feature],\n",
        "                    'threshold': threshold,\n",
        "                    'deviation': (features[feature] - threshold) / threshold * 100\n",
        "                })\n",
        "\n",
        "        return sorted(critical, key=lambda x: x['deviation'], reverse=True)[:5]\n",
        "\n",
        "    def learn_from_failure(self, device_id, failure_data):\n",
        "        \"\"\"Learn from actual failure events to improve predictions\"\"\"\n",
        "        if device_id not in self.health_history:\n",
        "            return\n",
        "\n",
        "        # Extract features leading to failure\n",
        "        pre_failure_data = self.health_history[device_id][-50:]  # Last 50 readings before failure\n",
        "\n",
        "        # Update failure patterns\n",
        "        failure_type = failure_data.get('failure_type', 'unknown')\n",
        "\n",
        "        if failure_type not in self.failure_patterns:\n",
        "            self.failure_patterns[failure_type] = {\n",
        "                'features': [],\n",
        "                'thresholds': {},\n",
        "                'weight': 0.8,\n",
        "                'maintenance_action': 'Investigate and repair'\n",
        "            }\n",
        "\n",
        "        # Analyze feature trends before failure\n",
        "        for feature_name in pre_failure_data[0]['features'].keys():\n",
        "            values = [d['features'][feature_name] for d in pre_failure_data if feature_name in d['features']]\n",
        "\n",
        "            if len(values) > 5:\n",
        "                mean_val = np.mean(values)\n",
        "                max_val = np.max(values)\n",
        "\n",
        "                if feature_name not in self.failure_patterns[failure_type]['features']:\n",
        "                    self.failure_patterns[failure_type]['features'].append(feature_name)\n",
        "\n",
        "                # Update threshold\n",
        "                self.failure_patterns[failure_type]['thresholds'][feature_name] = max_val * 0.8\n",
        "\n",
        "        logger.info(f\"Learned from failure: {failure_type} for device {device_id}\")\n",
        "\n",
        "class InteractiveDashboard:\n",
        "    \"\"\"Interactive dashboard for system visualization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.figures = {}\n",
        "        self.current_data = None\n",
        "\n",
        "    def create_dashboard(self, analysis_results, device_profiles):\n",
        "        \"\"\"Create comprehensive interactive dashboard\"\"\"\n",
        "\n",
        "        # Create subplot layout\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=3,\n",
        "            subplot_titles=(\n",
        "                'Device Health Status', 'Fault Probability Distribution',\n",
        "                'Feature Importance', 'Health Trend Over Time',\n",
        "                'Failure Pattern Analysis', 'Frequency Spectrum',\n",
        "                'Device Comparison', 'Maintenance Recommendations',\n",
        "                'Real-time Monitoring'\n",
        "            ),\n",
        "            specs=[\n",
        "                [{\"type\": \"indicator\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "                [{\"type\": \"scatter\"}, {\"type\": \"pie\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"heatmap\"}, {\"type\": \"table\"}, {\"type\": \"scatter\"}]\n",
        "            ],\n",
        "            vertical_spacing=0.08,\n",
        "            horizontal_spacing=0.08\n",
        "        )\n",
        "\n",
        "        # 1. Device Health Status (Gauge)\n",
        "        fig.add_trace(\n",
        "            go.Indicator(\n",
        "                mode=\"gauge+number+delta\",\n",
        "                value=analysis_results['health_score'],\n",
        "                title={'text': \"Health Score\"},\n",
        "                domain={'row': 0, 'column': 0},\n",
        "                gauge={\n",
        "                    'axis': {'range': [0, 100]},\n",
        "                    'bar': {'color': \"darkblue\"},\n",
        "                    'steps': [\n",
        "                        {'range': [0, 50], 'color': \"red\"},\n",
        "                        {'range': [50, 70], 'color': \"orange\"},\n",
        "                        {'range': [70, 85], 'color': \"yellow\"},\n",
        "                        {'range': [85, 100], 'color': \"green\"}\n",
        "                    ],\n",
        "                    'threshold': {\n",
        "                        'line': {'color': \"red\", 'width': 4},\n",
        "                        'thickness': 0.75,\n",
        "                        'value': 60\n",
        "                    }\n",
        "                }\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # 2. Fault Probability Distribution\n",
        "        # Check if analysis_results is a list (from batch_analyze) or a single dict\n",
        "        if isinstance(analysis_results, list):\n",
        "            fault_probs = [r['fault_probability'] for r in analysis_results]\n",
        "        else:\n",
        "            fault_probs = [analysis_results['fault_probability']]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Histogram(\n",
        "                x=fault_probs,\n",
        "                nbinsx=20,\n",
        "                name=\"Fault Probability\",\n",
        "                marker_color='coral'\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Feature Importance\n",
        "        if 'feature_importance' in analysis_results and analysis_results['feature_importance']:\n",
        "            features = list(analysis_results['feature_importance'].keys())[:10]\n",
        "            importance = list(analysis_results['feature_importance'].values())[:10]\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=importance,\n",
        "                    y=features,\n",
        "                    orientation='h',\n",
        "                    marker_color='lightseagreen'\n",
        "                ),\n",
        "                row=1, col=3\n",
        "            )\n",
        "\n",
        "        # 4. Health Trend Over Time\n",
        "        if 'health_history' in analysis_results and analysis_results['health_history']:\n",
        "            history = analysis_results['health_history']\n",
        "            timestamps = [h['timestamp'] for h in history[-50:]]\n",
        "            scores = [h['health_score'] for h in history[-50:]]\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=timestamps,\n",
        "                    y=scores,\n",
        "                    mode='lines+markers',\n",
        "                    name='Health Trend',\n",
        "                    line=dict(color='green', width=2)\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "\n",
        "        # 5. Failure Pattern Analysis\n",
        "        if 'matched_failures' in analysis_results and analysis_results['matched_failures']:\n",
        "            failures = analysis_results['matched_failures']\n",
        "            if failures:\n",
        "                labels = [f['failure_type'] for f in failures]\n",
        "                values = [f['confidence'] for f in failures]\n",
        "\n",
        "                fig.add_trace(\n",
        "                    go.Pie(\n",
        "                        labels=labels,\n",
        "                        values=values,\n",
        "                        hole=0.4,\n",
        "                        marker_colors=px.colors.sequential.RdBu\n",
        "                    ),\n",
        "                    row=2, col=2\n",
        "                )\n",
        "\n",
        "        # 6. Frequency Spectrum\n",
        "        if 'frequency_spectrum' in analysis_results and analysis_results['frequency_spectrum']:\n",
        "            freq_data = analysis_results['frequency_spectrum']\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=freq_data['frequencies'],\n",
        "                    y=freq_data['magnitude'],\n",
        "                    mode='lines',\n",
        "                    name='Spectrum',\n",
        "                    line=dict(color='purple', width=1)\n",
        "                ),\n",
        "                row=2, col=3\n",
        "            )\n",
        "\n",
        "        # 7. Device Comparison (Heatmap)\n",
        "        if 'device_comparison' in analysis_results and analysis_results['device_comparison']:\n",
        "            comp_data = analysis_results['device_comparison']\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Heatmap(\n",
        "                    z=comp_data['matrix'],\n",
        "                    x=comp_data['devices'],\n",
        "                    y=comp_data['features'],\n",
        "                    colorscale='Viridis'\n",
        "                ),\n",
        "                row=3, col=1\n",
        "            )\n",
        "\n",
        "        # 8. Maintenance Recommendations (Table)\n",
        "        if 'recommendations' in analysis_results and analysis_results['recommendations']:\n",
        "            recs = analysis_results['recommendations']\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Table(\n",
        "                    header=dict(\n",
        "                        values=['Priority', 'Action', 'Reason'],\n",
        "                        fill_color='paleturquoise',\n",
        "                        align='left'\n",
        "                    ),\n",
        "                    cells=dict(\n",
        "                        values=[\n",
        "                            [r['priority'] for r in recs],\n",
        "                            [r['action'] for r in recs],\n",
        "                            [r['reason'] for r in recs]\n",
        "                        ],\n",
        "                        fill_color='lavender',\n",
        "                        align='left'\n",
        "                    )\n",
        "                ),\n",
        "                row=3, col=2\n",
        "            )\n",
        "\n",
        "        # 9. Real-time Monitoring\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[0], y=[0],\n",
        "                mode='markers',\n",
        "                marker=dict(size=20, color=['green']),\n",
        "                name='Real-time Status'\n",
        "            ),\n",
        "            row=3, col=3\n",
        "        )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=1200,\n",
        "            showlegend=True,\n",
        "            title_text=\"Electrical Device Intelligence Dashboard\",\n",
        "            title_font_size=24\n",
        "        )\n",
        "\n",
        "        # Save dashboard\n",
        "        fig.write_html(\"dashboard.html\")\n",
        "\n",
        "        self.figures['dashboard'] = fig\n",
        "        return fig\n",
        "\n",
        "    def create_device_profile_card(self, device_profile):\n",
        "        \"\"\"Create device profile visualization card\"\"\"\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Device information\n",
        "        info_text = f\"\"\"\n",
        "        <b>Device:</b> {device_profile.name}<br>\n",
        "        <b>ID:</b> {device_profile.device_id}<br>\n",
        "        <b>Manufacturer:</b> {device_profile.manufacturer}<br>\n",
        "        <b>Power Rating:</b> {device_profile.power_rating} W<br>\n",
        "        <b>Age:</b> {device_profile.age_months} months<br>\n",
        "        <b>Operational Hours:</b> {device_profile.operational_hours:.0f}<br>\n",
        "        <b>Last Maintenance:</b> {device_profile.last_maintenance.strftime('%Y-%m-%d') if device_profile.last_maintenance else 'Never'}\n",
        "        \"\"\"\n",
        "\n",
        "        fig.add_annotation(\n",
        "            text=info_text,\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.5, y=0.5,\n",
        "            showarrow=False,\n",
        "            font=dict(size=14),\n",
        "            align=\"left\",\n",
        "            bgcolor=\"lightblue\"\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=f\"Device Profile: {device_profile.name}\",\n",
        "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "\n",
        "        self.figures[f'profile_{device_profile.device_id}'] = fig\n",
        "        return fig\n",
        "\n",
        "    def create_health_timeline(self, health_history):\n",
        "        \"\"\"Create health timeline visualization\"\"\"\n",
        "        if not health_history:\n",
        "            return None\n",
        "\n",
        "        timestamps = [h['timestamp'] for h in health_history]\n",
        "        scores = [h['health_score'] for h in health_history]\n",
        "        statuses = [h.get('status', 'UNKNOWN') for h in health_history]\n",
        "\n",
        "        # Color mapping for status\n",
        "        color_map = {\n",
        "            'HEALTHY': 'green',\n",
        "            'NORMAL': 'lightgreen',\n",
        "            'ATTENTION': 'yellow',\n",
        "            'WARNING': 'orange',\n",
        "            'CRITICAL': 'red'\n",
        "        }\n",
        "\n",
        "        colors = [color_map.get(s, 'gray') for s in statuses]\n",
        "\n",
        "        fig = go.Figure()\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=timestamps,\n",
        "            y=scores,\n",
        "            mode='lines+markers',\n",
        "            name='Health Score',\n",
        "            line=dict(color='blue', width=2),\n",
        "            marker=dict(color=colors, size=8),\n",
        "            text=statuses,\n",
        "            hoverinfo='x+y+text'\n",
        "        ))\n",
        "\n",
        "        # Add threshold lines\n",
        "        fig.add_hline(y=80, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Healthy Threshold\")\n",
        "        fig.add_hline(y=60, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"Warning Threshold\")\n",
        "        fig.add_hline(y=40, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Critical Threshold\")\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=\"Device Health Timeline\",\n",
        "            xaxis_title=\"Time\",\n",
        "            yaxis_title=\"Health Score\",\n",
        "            yaxis_range=[0, 100],\n",
        "            hovermode='x unified'\n",
        "        )\n",
        "\n",
        "        self.figures['health_timeline'] = fig\n",
        "        return fig\n",
        "\n",
        "class AdvancedElectricalAnalyzer:\n",
        "    \"\"\"Main system controller with all advanced features\"\"\"\n",
        "\n",
        "    def __init__(self, use_ml=True, use_deep_learning=False):\n",
        "        self.processor = AdvancedSignalProcessor()\n",
        "        self.ml_identifier = MLDeviceIdentifier('ensemble' if use_ml else 'rules')\n",
        "        self.failure_predictor = AdaptiveFailurePredictor()\n",
        "        self.dashboard = InteractiveDashboard()\n",
        "        self.device_profiles = {}\n",
        "        self.data_log = []\n",
        "        self.use_ml = use_ml\n",
        "        self.use_deep_learning = use_deep_learning and DL_AVAILABLE\n",
        "\n",
        "        # Create data directory\n",
        "        Path(\"data\").mkdir(exist_ok=True)\n",
        "        Path(\"models\").mkdir(exist_ok=True)\n",
        "        Path(\"reports\").mkdir(exist_ok=True)\n",
        "\n",
        "        logger.info(\"Advanced Electrical Analyzer Initialized\")\n",
        "\n",
        "    def register_device(self, device_profile):\n",
        "        \"\"\"Register a new device in the system\"\"\"\n",
        "        self.device_profiles[device_profile.device_id] = device_profile\n",
        "        logger.info(f\"Device registered: {device_profile.name} ({device_profile.device_id})\")\n",
        "\n",
        "    def analyze_device_signal(self, signal, device_id=None, context=None):\n",
        "        \"\"\"Complete analysis of device signal\"\"\"\n",
        "\n",
        "        # Process signal\n",
        "        processed_signal = self.processor.process_pipeline(signal, method='advanced')\n",
        "\n",
        "        # Extract features\n",
        "        features = self.processor.extract_comprehensive_features(processed_signal)\n",
        "\n",
        "        # Device identification\n",
        "        if self.use_ml:\n",
        "            device, confidence, all_predictions, probabilities = self.ml_identifier.predict(signal)\n",
        "        else:\n",
        "            # Fallback to rule-based\n",
        "            device, confidence, _ = self.ml_identifier.identify_device(signal)\n",
        "            all_predictions = {'rule_based': device}\n",
        "            probabilities = {'rule_based': {device: confidence}}\n",
        "\n",
        "        # Health analysis\n",
        "        device_profile = self.device_profiles.get(device_id)\n",
        "        historical_data = self._get_device_history(device_id) if device_id else None\n",
        "\n",
        "        health_analysis = self.failure_predictor.analyze_health(\n",
        "            features, device_id or 'unknown', historical_data\n",
        "        )\n",
        "\n",
        "        # Create result object\n",
        "        result = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'device_id': device_id,\n",
        "            'identified_device': device,\n",
        "            'identification_confidence': confidence,\n",
        "            'all_predictions': all_predictions,\n",
        "            'probabilities': probabilities,\n",
        "            'features': features,\n",
        "            'health_analysis': health_analysis,\n",
        "            'signal_info': {\n",
        "                'length': len(signal),\n",
        "                'sampling_rate': self.processor.sampling_rate,\n",
        "                'processing_method': 'advanced'\n",
        "            },\n",
        "            'context': context\n",
        "        }\n",
        "\n",
        "        # Log result\n",
        "        self.data_log.append(result)\n",
        "\n",
        "        # Save to database\n",
        "        self._save_to_database(result)\n",
        "\n",
        "        # Generate report\n",
        "        report = self._generate_report(result)\n",
        "\n",
        "        # Update dashboard\n",
        "        if device_id in self.device_profiles:\n",
        "            self.dashboard.create_device_profile_card(self.device_profiles[device_id])\n",
        "\n",
        "        return result, report\n",
        "\n",
        "    def batch_analyze(self, signals, device_ids=None):\n",
        "        \"\"\"Batch analysis of multiple signals\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i, signal in enumerate(signals):\n",
        "            device_id = device_ids[i] if device_ids and i < len(device_ids) else None\n",
        "            result, _ = self.analyze_device_signal(signal, device_id)\n",
        "            results.append(result)\n",
        "\n",
        "            # Progress update\n",
        "            if (i + 1) % 10 == 0:\n",
        "                logger.info(f\"Processed {i + 1}/{len(signals)} signals\")\n",
        "\n",
        "        # Create comparative analysis\n",
        "        comparative = self._create_comparative_analysis(results)\n",
        "\n",
        "        return results, comparative\n",
        "\n",
        "    def train_ml_models(self, training_data, labels):\n",
        "        \"\"\"Train machine learning models\"\"\"\n",
        "        logger.info(f\"Training ML models on {len(training_data)} samples\")\n",
        "\n",
        "        # Convert signals to features\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for signal, label in zip(training_data, labels):\n",
        "            features = self.processor.extract_comprehensive_features(signal)\n",
        "            X.append(list(features.values()))\n",
        "            y.append(label)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Train models\n",
        "        self.ml_identifier.train(X, y)\n",
        "\n",
        "        # Save trained models\n",
        "        self.ml_identifier.save_models()\n",
        "\n",
        "        logger.info(\"ML models trained and saved successfully\")\n",
        "\n",
        "    def real_time_monitoring(self, data_stream, device_id, duration_seconds=3600):\n",
        "        \"\"\"Real-time monitoring of device\"\"\"\n",
        "        logger.info(f\"Starting real-time monitoring for device {device_id}\")\n",
        "\n",
        "        monitoring_data = []\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        try:\n",
        "            while (datetime.now() - start_time).seconds < duration_seconds:\n",
        "                # Get latest signal from stream\n",
        "                signal = data_stream.get_latest()\n",
        "\n",
        "                if signal is not None:\n",
        "                    result, _ = self.analyze_device_signal(signal, device_id, context='real_time')\n",
        "                    monitoring_data.append(result)\n",
        "\n",
        "                    # Check for critical conditions\n",
        "                    if result['health_analysis']['status'] == 'CRITICAL':\n",
        "                        self._send_alert(device_id, result)\n",
        "\n",
        "                    # Update dashboard\n",
        "                    self._update_real_time_dashboard(result)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.info(\"Monitoring stopped by user\")\n",
        "\n",
        "        # Generate monitoring report\n",
        "        report = self._generate_monitoring_report(monitoring_data, device_id)\n",
        "\n",
        "        return monitoring_data, report\n",
        "\n",
        "    def _get_device_history(self, device_id):\n",
        "        \"\"\"Get historical data for device\"\"\"\n",
        "        return [r['health_analysis'] for r in self.data_log\n",
        "                if r.get('device_id') == device_id]\n",
        "\n",
        "    def _save_to_database(self, result):\n",
        "        \"\"\"Save result to database (simplified)\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"data/analysis_{timestamp}_{result.get('device_id', 'unknown')}.json\"\n",
        "\n",
        "        # Convert to serializable format\n",
        "        serializable = {}\n",
        "        for key, value in result.items():\n",
        "            if isinstance(value, (np.ndarray, np.generic)):\n",
        "                serializable[key] = value.tolist()\n",
        "            elif isinstance(value, datetime):\n",
        "                serializable[key] = value.isoformat()\n",
        "            else:\n",
        "                serializable[key] = value\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(serializable, f, indent=2)\n",
        "\n",
        "    def _generate_report(self, result):\n",
        "        \"\"\"Generate analysis report\"\"\"\n",
        "        report = {\n",
        "            'summary': {\n",
        "                'device': result['identified_device'],\n",
        "                'confidence': result['identification_confidence'],\n",
        "                'status': result['health_analysis']['status'],\n",
        "                'health_score': result['health_analysis']['health_score'],\n",
        "                'fault_probability': result['health_analysis']['fault_probability']\n",
        "            },\n",
        "            'detailed_analysis': result['health_analysis'],\n",
        "            'features': result['features'],\n",
        "            'recommendations': result['health_analysis']['recommendations'],\n",
        "            'timestamp': result['timestamp'].isoformat()\n",
        "        }\n",
        "\n",
        "        # Save report\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"reports/report_{timestamp}_{result.get('device_id', 'unknown')}.json\"\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        return report\n",
        "\n",
        "    def _create_comparative_analysis(self, results):\n",
        "        \"\"\"Create comparative analysis of multiple devices\"\"\"\n",
        "        comparative = {\n",
        "            'device_stats': {},\n",
        "            'health_comparison': [],\n",
        "            'feature_correlation': {}\n",
        "        }\n",
        "\n",
        "        for result in results:\n",
        "            device_id = result.get('device_id', 'unknown')\n",
        "            comparative['device_stats'][device_id] = {\n",
        "                'health_score': result['health_analysis']['health_score'],\n",
        "                'fault_probability': result['health_analysis']['fault_probability'],\n",
        "                'status': result['health_analysis']['status']\n",
        "            }\n",
        "\n",
        "        return comparative\n",
        "\n",
        "    def _send_alert(self, device_id, result):\n",
        "        \"\"\"Send alert for critical condition\"\"\"\n",
        "        alert = {\n",
        "            'device_id': device_id,\n",
        "            'device_name': self.device_profiles.get(device_id, {}).get('name', 'Unknown'),\n",
        "            'status': result['health_analysis']['status'],\n",
        "            'health_score': result['health_analysis']['health_score'],\n",
        "            'fault_probability': result['health_analysis']['fault_probability'],\n",
        "            'critical_issues': result['health_analysis'].get('matched_failures', []),\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'urgency': 'IMMEDIATE' if result['health_analysis']['status'] == 'CRITICAL' else 'HIGH'\n",
        "        }\n",
        "\n",
        "        # Log alert\n",
        "        logger.critical(f\"ALERT: {device_id} - {result['health_analysis']['status']}\")\n",
        "\n",
        "        # Save alert\n",
        "        with open(f\"alerts/alert_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\", 'w') as f:\n",
        "            json.dump(alert, f, indent=2)\n",
        "\n",
        "        # Here you would integrate with email/SMS/notification system\n",
        "        return alert\n",
        "\n",
        "    def _update_real_time_dashboard(self, result):\n",
        "        \"\"\"Update real-time dashboard\"\"\"\n",
        "        # This would update a live dashboard\n",
        "        pass\n",
        "\n",
        "    def _generate_monitoring_report(self, monitoring_data, device_id):\n",
        "        \"\"\"Generate monitoring session report\"\"\"\n",
        "        if not monitoring_data:\n",
        "            return None\n",
        "\n",
        "        report = {\n",
        "            'device_id': device_id,\n",
        "            'duration_seconds': len(monitoring_data),\n",
        "            'start_time': monitoring_data[0]['timestamp'].isoformat(),\n",
        "            'end_time': monitoring_data[-1]['timestamp'].isoformat(),\n",
        "            'health_trend': {\n",
        "                'initial_score': monitoring_data[0]['health_analysis']['health_score'],\n",
        "                'final_score': monitoring_data[-1]['health_analysis']['health_score'],\n",
        "                'average_score': np.mean([d['health_analysis']['health_score'] for d in monitoring_data]),\n",
        "                'min_score': np.min([d['health_analysis']['health_score'] for d in monitoring_data]),\n",
        "                'max_score': np.max([d['health_analysis']['health_score'] for d in monitoring_data])\n",
        "            },\n",
        "            'alerts_generated': sum(1 for d in monitoring_data if d['health_analysis']['status'] in ['WARNING', 'CRITICAL']),\n",
        "            'recommendations': monitoring_data[-1]['health_analysis']['recommendations']\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "    def generate_synthetic_data(self, device_types=None, num_samples=1000):\n",
        "        \"\"\"Generate synthetic training data\"\"\"\n",
        "        if device_types is None:\n",
        "            device_types = ['Refrigerator', 'Air Conditioner', 'Washing Machine',\n",
        "                          'LED Light', 'Microwave Oven', 'Industrial Motor']\n",
        "\n",
        "        signals = []\n",
        "        labels = []\n",
        "\n",
        "        for device in device_types:\n",
        "            for _ in range(num_samples // len(device_types)):\n",
        "                # Generate varying signals for each device type\n",
        "                signal = self._generate_synthetic_signal(device)\n",
        "                signals.append(signal)\n",
        "                labels.append(device)\n",
        "\n",
        "        return np.array(signals), np.array(labels)\n",
        "\n",
        "    def _generate_synthetic_signal(self, device_type):\n",
        "        \"\"\"Generate synthetic signal for a device type\"\"\"\n",
        "        duration = 0.1  # 100ms\n",
        "        t = np.linspace(0, duration, int(50000 * duration))\n",
        "\n",
        "        # Base signal\n",
        "        if device_type == \"Refrigerator\":\n",
        "            base = 2.0 * np.sin(2 * np.pi * 50 * t)\n",
        "            harmonics = [0.3, 0.1, 0.05]\n",
        "            noise_level = 0.1\n",
        "        elif device_type == \"Air Conditioner\":\n",
        "            base = 6.0 * np.sin(2 * np.pi * 50 * t)\n",
        "            harmonics = [0.8, 0.3, 0.1]\n",
        "            noise_level = 0.15\n",
        "        elif device_type == \"Washing Machine\":\n",
        "            base = 3.0 * np.sin(2 * np.pi * 50 * t)\n",
        "            harmonics = [0.4, 0.2, 0.1]\n",
        "            noise_level = 0.2\n",
        "        elif device_type == \"LED Light\":\n",
        "            base = 0.3 * np.sin(2 * np.pi * 1000 * t)\n",
        "            harmonics = [0.1, 0.05, 0.02]\n",
        "            noise_level = 0.05\n",
        "        elif device_type == \"Microwave Oven\":\n",
        "            base = 5.0 * np.sin(2 * np.pi * 50 * t)\n",
        "            harmonics = [0.6, 0.3, 0.15]\n",
        "            noise_level = 0.25\n",
        "        elif device_type == \"Industrial Motor\":\n",
        "            base = 10.0 * np.sin(2 * np.pi * 50 * t)\n",
        "            harmonics = [1.5, 0.8, 0.4]\n",
        "            noise_level = 0.3\n",
        "        else:\n",
        "            base = 1.0 * np.sin(2 * np.pi * 50 * t)\n",
        "            harmonics = [0.1, 0.05, 0.02]\n",
        "            noise_level = 0.1\n",
        "\n",
        "        # Add harmonics\n",
        "        signal = base\n",
        "        for i, amp in enumerate(harmonics, start=2):\n",
        "            signal += amp * np.sin(2 * np.pi * 50 * i * t)\n",
        "\n",
        "        # Add noise\n",
        "        signal += noise_level * np.random.randn(len(t))\n",
        "\n",
        "        # Add random transients (10% chance)\n",
        "        if np.random.random() < 0.1:\n",
        "            transient_pos = np.random.randint(len(t) // 4, 3 * len(t) // 4)\n",
        "            transient_len = np.random.randint(10, 50)\n",
        "            signal[transient_pos:transient_pos + transient_len] += 3 * noise_level\n",
        "\n",
        "        return signal\n",
        "\n",
        "# Example usage and demonstration\n",
        "def main():\n",
        "    \"\"\"Main demonstration function\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ADVANCED ELECTRICAL DEVICE INTELLIGENCE SYSTEM - DEMONSTRATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Initialize system\n",
        "    analyzer = AdvancedElectricalAnalyzer(use_ml=True, use_deep_learning=DL_AVAILABLE)\n",
        "\n",
        "    # Register some devices\n",
        "    devices = [\n",
        "        DeviceProfile(\n",
        "            name=\"Kitchen Refrigerator\",\n",
        "            device_id=\"DEV001\",\n",
        "            manufacturer=\"Samsung\",\n",
        "            power_rating=150,\n",
        "            voltage_rating=220,\n",
        "            current_rating=0.68,\n",
        "            age_months=24,\n",
        "            last_maintenance=datetime.now() - timedelta(days=60),\n",
        "            operational_hours=17520\n",
        "        ),\n",
        "        DeviceProfile(\n",
        "            name=\"Living Room AC\",\n",
        "            device_id=\"DEV002\",\n",
        "            manufacturer=\"LG\",\n",
        "            power_rating=2000,\n",
        "            voltage_rating=220,\n",
        "            current_rating=9.1,\n",
        "            age_months=12,\n",
        "            last_maintenance=datetime.now() - timedelta(days=30),\n",
        "            operational_hours=4380\n",
        "        ),\n",
        "        DeviceProfile(\n",
        "            name=\"Industrial Motor 5HP\",\n",
        "            device_id=\"DEV003\",\n",
        "            manufacturer=\"Siemens\",\n",
        "            power_rating=3728,\n",
        "            voltage_rating=380,\n",
        "            current_rating=7.5,\n",
        "            age_months=36,\n",
        "            last_maintenance=datetime.now() - timedelta(days=90),\n",
        "            operational_hours=26280\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    for device in devices:\n",
        "        analyzer.register_device(device)\n",
        "\n",
        "    print(f\"\\nRegistered {len(devices)} devices\")\n",
        "\n",
        "    # Generate synthetic training data\n",
        "    print(\"\\nGenerating synthetic training data...\")\n",
        "    signals, labels = analyzer.generate_synthetic_data(num_samples=600)\n",
        "\n",
        "    # Train ML models\n",
        "    print(\"Training machine learning models...\")\n",
        "    analyzer.train_ml_models(signals[:500], labels[:500])\n",
        "\n",
        "    # Test the system\n",
        "    print(\"\\nTesting system with sample signals...\")\n",
        "    test_results = []\n",
        "\n",
        "    for i in range(min(5, len(signals[500:]))):\n",
        "        signal = signals[500 + i]\n",
        "        actual_label = labels[500 + i]\n",
        "\n",
        "        result, report = analyzer.analyze_device_signal(signal, context=\"test\")\n",
        "        test_results.append({\n",
        "            'actual': actual_label,\n",
        "            'predicted': result['identified_device'],\n",
        "            'confidence': result['identification_confidence'],\n",
        "            'health': result['health_analysis']['health_score']\n",
        "        })\n",
        "\n",
        "        print(f\"\\nTest {i+1}:\")\n",
        "        print(f\"  Actual: {actual_label}\")\n",
        "        print(f\"  Predicted: {result['identified_device']} ({result['identification_confidence']:.1%})\")\n",
        "        print(f\"  Health Score: {result['health_analysis']['health_score']:.1f}/100\")\n",
        "        print(f\"  Status: {result['health_analysis']['status']}\")\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct = sum(1 for r in test_results if r['actual'] == r['predicted'])\n",
        "    accuracy = correct / len(test_results) * 100\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"TEST RESULTS: {accuracy:.1f}% accuracy\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Generate comprehensive dashboard\n",
        "    print(\"\\nGenerating interactive dashboard...\")\n",
        "    dashboard_data = {\n",
        "        'health_score': test_results[0]['health'] if test_results else 75,\n",
        "        'fault_probability': 0.25,\n",
        "        'feature_importance': {'RMS': 0.3, 'Crest_Factor': 0.25, 'THD': 0.2, 'Kurtosis': 0.15, 'Entropy': 0.1},\n",
        "        'health_history': analyzer.data_log[-20:] if analyzer.data_log else [],\n",
        "        'matched_failures': [],\n",
        "        'frequency_spectrum': {'frequencies': np.linspace(0, 1000, 100), 'magnitude': np.random.rand(100)},\n",
        "        'device_comparison': {\n",
        "            'devices': ['Dev1', 'Dev2', 'Dev3'],\n",
        "            'features': ['Health', 'Power', 'Age'],\n",
        "            'matrix': np.random.rand(3, 3)\n",
        "        },\n",
        "        'recommendations': [\n",
        "            {'priority': 'MEDIUM', 'action': 'Schedule maintenance', 'reason': 'Normal wear'},\n",
        "            {'priority': 'LOW', 'action': 'Clean filters', 'reason': 'Routine maintenance'}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    analyzer.dashboard.create_dashboard(dashboard_data, devices)\n",
        "\n",
        "    # Create health timeline for first device\n",
        "    if devices and analyzer.data_log:\n",
        "        device_logs = [log for log in analyzer.data_log if log.get('device_id') == devices[0].device_id]\n",
        "        if device_logs:\n",
        "            health_history = [{\n",
        "                'timestamp': log['timestamp'],\n",
        "                'health_score': log['health_analysis']['health_score'],\n",
        "                'status': log['health_analysis']['status']\n",
        "            } for log in device_logs[-50:]]\n",
        "\n",
        "            analyzer.dashboard.create_health_timeline(health_history)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYSTEM READY\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nGenerated Files:\")\n",
        "    print(\"   electrical_analysis.log - System logs\")\n",
        "    print(\"   dashboard.html - Interactive dashboard\")\n",
        "    print(\"   models/ - Trained ML models\")\n",
        "    print(\"   data/ - Analysis results\")\n",
        "    print(\"   reports/ - Detailed reports\")\n",
        "    print(\"\\nNext Steps:\")\n",
        "    print(\"  1. Connect real sensors for data collection\")\n",
        "    print(\"  2. Integrate with building management system\")\n",
        "    print(\"  3. Set up alert notifications (email/SMS)\")\n",
        "    print(\"  4. Deploy to production environment\")\n",
        "\n",
        "    return analyzer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the system\n",
        "    system = main()\n",
        "\n",
        "    # Keep the system running for demonstration\n",
        "    print(\"\\nSystem is running. Press Ctrl+C to exit.\")\n",
        "\n",
        "    try:\n",
        "        import time\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nShutting down system...\")\n",
        "        print(\"Thank you for using the Advanced Electrical Device Intelligence System!\")"
      ]
    }
  ]
}